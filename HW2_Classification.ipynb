{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  &#x1F4D1; **作业 2: 音位分类 (分类)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学习目标：\n",
    "* 数据预处理：从原始波形中提取MFCC特征\n",
    "* 分类：使用预提取的MFCC特征执行逐帧音位分类\n",
    "* 熟悉并提高pytorch训练技巧，熟悉pytorch模块\n",
    "相关资料：\n",
    "* Slides地址: https://docs.google.com/presentation/d/1v6HkBWiJb8WNDcJ9_-2kwVstxUWml87b9CnA16Gdoio/edit?usp=sharing\n",
    "* Kaggle地址: https://www.kaggle.com/c/ml2022spring-hw2\n",
    "* 相关课程视频资源也可在B站获取  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前置知识：\n",
    "- 音位：\n",
    "\n",
    "(phonetics 语音) 音位，音素（区分单词的最小语音单位，英语sip中的s和zip中的z是两个不同的音素）  \n",
    "\n",
    "例如：Machine Learning → M AH SH IH N L ER N IH NG M M M AH AH SH SH IH IH IH N N N N ... Machine\n",
    "\n",
    "- MFCC：  \n",
    "\n",
    "在语音识别（Speech recognition）和话者识别（Speaker recognition）方面，最常用到的语音特征就是梅尔倒谱系数（Mel-scale Frequency Cepstral Coefficients, MFCC）。  \n",
    "\n",
    "根据人耳听觉机理的研究发现，人耳对不同频率的声波有不同的听觉敏感度。从200Hz到5000Hz的语音信号对语音的清晰度影响对大。两个响度不等的声音作用于人耳时，则响度较高的频率成分的存在会影响到对响度较低的频率成分的感受，使其变得不易察觉，这种现象称为掩蔽效应。由于频率较低的声音在内耳蜗基底膜上行波传递的距离（速度）大于频率较高的声音，故一般来说，低音容易掩蔽高音，而高音掩蔽低音较困难。在低频处的声音掩蔽的临界带宽较高频要小。所以，人们从低频到高频这一段频带内按临界带宽的大小由密到疏安排一组带通滤波器，对输入信号进行滤波。将每个带通滤波器输出的信号能量作为信号的基本特征，对此特征经过进一步处理后就可以作为语音的输入特征。由于这种特征不依赖于信号的性质，对输入信号不做任何的假设和限制，又利用了听觉模型的研究成果。因此，这种参数比基于声道模型的LPCC相比具有更好的鲁棒性，更符合人耳的听觉特性，而且当信噪比降低时仍然具有较好的识别性能。  \n",
    "\n",
    "在本次作业中，正常的一段音频素材可能包含大量的音位信息，而音位之间又可能存在重叠干扰的情况，因此我们将一段音频素材每隔10ms切取25ms，以此来尽可能保存完整的音位素材，取出后的素材称为一个frame，取出后的frame并不适合直接进入训练，因此我们要进行进一步的处理，通过MFCC，将它转化为一个39维度的特征，转换后为了更加精确的判断当前特征内的音位信息，我们往往采取其前后的特征来做辅助判断 也就是前向特征与后向特征各取5个，所以我们最后得到的是一个11*39维的一个向量。\n",
    "\n",
    "![](./pic/01.png) \n",
    "想要深入了解实现过程的可以查看下列链接：  \n",
    "[Prof. Hung-Yi Lee[2020Spring DLHLP] Speech Recognition](https://speech.ee.ntu.edu.tw/~tlkagk/courses/DLHLP20/ASR%20(v12).pdf)  \n",
    "[ Prof. Lin-Shan Lee’s[Introduction to Digital Speech Processing]Chap.7](http://ocw.aca.ntu.edu.tw/ntu-ocw/ocw/cou/104S204)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  数据集下载\n",
    "如果下列命令无法下载，可以到下列地址下载数据\n",
    "- Kaggle下载数据:  [Kaggle: ml2022spring-hw2](https://www.kaggle.com/competitions/ml2022spring-hw2)\n",
    "- 百度云下载数据: [云盘(提取码：05zc)](https://pan.baidu.com/s/198xn8Lk9MjvUsq866mZuuw)\n",
    "\n",
    "\n",
    "下载完成后，你应该能够获取如下文件：\n",
    "- `libriphone/train_split.txt`\n",
    "- `libriphone/train_labels`\n",
    "- `libriphone/test_split.txt`\n",
    "- `libriphone/feat/train/*.pt`: training feature<br>\n",
    "- `libriphone/feat/test/*.pt`:  testing feature<br>  \n",
    "\n",
    "pt文件可以使用torch.load方法导入\n",
    "\n",
    "\n",
    "<b>同学们下载完后直接解压到 HW02文件夹下面（将里面的文件最终放到HW02下）</b>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:04:18.800341Z",
     "start_time": "2024-10-07T14:04:18.073282Z"
    }
   },
   "source": [
    "# 输入如下指令查看GPU状态\n",
    "!nvidia-smi"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct  7 22:04:18 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 537.42                 Driver Version: 537.42       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060 ...  WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   43C    P8              14W /   5W |    111MiB /  6144MiB |      2%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      5636    C+G   ...cal\\Microsoft\\OneDrive\\OneDrive.exe    N/A      |\n",
      "|    0   N/A  N/A     11168    C+G   D:\\todesk\\ToDesk.exe                      N/A      |\n",
      "|    0   N/A  N/A     11448    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A     13032    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A     15164    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper函数用于预处理来自每个话语的原始MFCC特征的训练数据**\n",
    "\n",
    "\n",
    "一个音位可能跨越几个帧，并且取决于过去和将来的帧\n",
    "\n",
    "因此，我们连接相邻的音位进行训练以获得更高的准确性。**concat_fatte**函数连接过去和未来的k帧（总共2k+1＝n帧），我们预测中心帧。\n",
    "\n",
    "\n",
    "可以随意修改数据预处理函数，但**不要删除任何帧**（如果您修改函数，请记住检查帧的数量是否与幻灯片中提到的相同）"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:07:05.720952Z",
     "start_time": "2024-10-07T14:06:59.501955Z"
    }
   },
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "#导入pytorch\n",
    "import torch\n",
    "#导入进度条\n",
    "from tqdm import tqdm\n",
    "# 定义导入feature函数\n",
    "def load_feat(path):\n",
    "    feat = torch.load(path)\n",
    "    return feat\n",
    "\n",
    "def shift(x, n):\n",
    "    if n < 0:\n",
    "        left = x[0].repeat(-n, 1)\n",
    "        right = x[:n]\n",
    "\n",
    "    elif n > 0:\n",
    "        right = x[-1].repeat(n, 1)\n",
    "        left = x[n:]\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "    return torch.cat((left, right), dim=0)\n",
    "# 将前后的特征联系在一起，如concat_n = 11 则前后都接5\n",
    "def concat_feat(x, concat_n):\n",
    "    assert concat_n % 2 == 1 # n 必须是奇数\n",
    "    if concat_n < 2:\n",
    "        return x\n",
    "    seq_len, feature_dim = x.size(0), x.size(1)\n",
    "    x = x.repeat(1, concat_n) \n",
    "    x = x.view(seq_len, concat_n, feature_dim).permute(1, 0, 2) # concat_n, seq_len, feature_dim\n",
    "    mid = (concat_n // 2)\n",
    "    for r_idx in range(1, mid+1):\n",
    "        x[mid + r_idx, :] = shift(x[mid + r_idx], r_idx)\n",
    "        x[mid - r_idx, :] = shift(x[mid - r_idx], -r_idx)\n",
    "\n",
    "    return x.permute(1, 0, 2).view(seq_len, concat_n * feature_dim)\n",
    "\n",
    "# 数据预处理函数\n",
    "def preprocess_data(split, feat_dir, phone_path, concat_nframes, train_ratio=0.8, train_val_seed=1337):\n",
    "    class_num = 41 # NOTE: 预先计算，不需要更改\n",
    "    mode = 'train' if (split == 'train' or split == 'val') else 'test'\n",
    "\n",
    "    label_dict = {}\n",
    "    if mode != 'test':\n",
    "      phone_file = open(os.path.join(phone_path, f'{mode}_labels.txt')).readlines()\n",
    "\n",
    "      for line in phone_file:\n",
    "          line = line.strip('\\n').split(' ')\n",
    "          label_dict[line[0]] = [int(p) for p in line[1:]]\n",
    "\n",
    "    if split == 'train' or split == 'val':\n",
    "        # 分割训练和验证数据\n",
    "        usage_list = open(os.path.join(phone_path, 'train_split.txt')).readlines()\n",
    "        random.seed(train_val_seed)\n",
    "        random.shuffle(usage_list)\n",
    "        percent = int(len(usage_list) * train_ratio)\n",
    "        usage_list = usage_list[:percent] if split == 'train' else usage_list[percent:]\n",
    "    elif split == 'test':\n",
    "        usage_list = open(os.path.join(phone_path, 'test_split.txt')).readlines()\n",
    "    else:\n",
    "        raise ValueError('Invalid \\'split\\' argument for dataset: PhoneDataset!')\n",
    "\n",
    "    usage_list = [line.strip('\\n') for line in usage_list]\n",
    "    print('[Dataset] - # phone classes: ' + str(class_num) + ', number of utterances for ' + split + ': ' + str(len(usage_list)))\n",
    "\n",
    "    max_len = 3000000\n",
    "    X = torch.empty(max_len, 39 * concat_nframes)\n",
    "    if mode != 'test':\n",
    "      y = torch.empty(max_len, dtype=torch.long)\n",
    "\n",
    "    idx = 0\n",
    "    for i, fname in tqdm(enumerate(usage_list)):\n",
    "        feat = load_feat(os.path.join(feat_dir, mode, f'{fname}.pt'))\n",
    "        cur_len = len(feat)\n",
    "        feat = concat_feat(feat, concat_nframes)\n",
    "        if mode != 'test':\n",
    "          label = torch.LongTensor(label_dict[fname])\n",
    "\n",
    "        X[idx: idx + cur_len, :] = feat\n",
    "        if mode != 'test':\n",
    "          y[idx: idx + cur_len] = label\n",
    "\n",
    "        idx += cur_len\n",
    "\n",
    "    X = X[:idx, :]\n",
    "    if mode != 'test':\n",
    "      y = y[:idx]\n",
    "\n",
    "    print(f'[INFO] {split} set')\n",
    "    print(X.shape)\n",
    "    if mode != 'test':\n",
    "      print(y.shape)\n",
    "      return X, y\n",
    "    else:\n",
    "      return X\n",
    "# 返回的X代表数据的维度，如果不链接则为39 如果链接即为n*39 n为连接的特征总数,y为标签"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\faceswap\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义数据集"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:07:17.475300Z",
     "start_time": "2024-10-07T14:07:17.453330Z"
    }
   },
   "source": [
    "import torch\n",
    "#导入数据集\n",
    "from torch.utils.data import Dataset\n",
    "#导入数据加载工具Dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "#定义数据集，一个数据集类应该包含初始化，_getitem__（获取一个元素）以及__len__（获取数据长度）方法\n",
    "class LibriDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.data = X\n",
    "        if y is not None:\n",
    "            self.label = torch.LongTensor(y)\n",
    "        else:\n",
    "            self.label = None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.label is not None:\n",
    "            return self.data[idx], self.label[idx]\n",
    "        else:\n",
    "            return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  &#x2728; 神经网络模型\n",
    "<font color=darkred><b>***TODO***: 使用近似相同数量的参数实现2个模型，（A）一个更窄和更深（例如hidden_layers=6，hidden_dim＝1024）和（B）另一个更宽和更浅（例如 hidden_layers＝2、hidden_dim＝1700）。报告两种模型的训练/验证精度。</font></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred><b>***TODO***:  添加dropout层，并报告dropout率分别等于（A）0.25/（B）0.5/（C）0.75的训练/验证准确性。</font></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout层在神经网络层当中是用来干什么的呢？它是一种可以用于减少神经网络过拟合的结构。\n",
    "![](./pic/02.png)   \n",
    "如上图我们定义的网络,一共有四个输入x_i，一个输出y。Dropout则是在每一个batch的训练当中随机减掉一些神经元，而作为编程者，我们可以设定每一层dropout（将神经元去除的的多少）的概率，在设定之后，就可以得到第一个batch进行训练的结果：  \n",
    "![](./pic/03.png)   \n",
    "从上图我们可以看到一些神经元之间断开了连接，因此它们被dropout了！dropout顾名思义就是被拿掉的意思，正因为我们在神经网络当中拿掉了一些神经元，所以才叫做dropout层。\n",
    "在进行第一个batch的训练时，有以下步骤：\n",
    "* 设定每一个神经网络层进行dropout的概率\n",
    "* 根据相应的概率拿掉一部分的神经元，然后开始训练，更新没有被拿掉神经元以及权重的参数，将其保留\n",
    "* 参数全部更新之后，又重新根据相应的概率拿掉一部分神经元，然后开始训练，如果新用于训练的神经元已经在第一次当中训练过，那么我们继续更新它的参数。而第二次被剪掉的神经元，同时第一次已经更新过参数的，我们保留它的权重，不做修改，直到第n次batch进行dropout时没有将其删除。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PS: 上面的两个TODO是可以改进其他部分来提高你的成绩的方法。  \n",
    "如下的策略是助教给出的几个优化方式：  \n",
    "● (1%) Simple baseline: 0.45797 (sample code)  \n",
    "● (1%) Medium baseline: 0.69747 (concat n frames, add layers)  \n",
    "● (1%) Strong baseline: 0.75028 (concat n, batchnorm, dropout, add layers)  \n",
    "● (1%) Boss baseline: 0.82324 (sequence-labeling(using RNN))  \n",
    "对于boss baseline，您可以参考RNN之前的课程记录"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:13:06.205599Z",
     "start_time": "2024-10-07T14:13:06.190640Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# 建立神经网络\n",
    "class BasicBlock(nn.Module):# 继承 torch 的 Module\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(input_dim, output_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=41, hidden_layers=1, hidden_dim=256):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            BasicBlock(input_dim, hidden_dim),\n",
    "            *[BasicBlock(hidden_dim, hidden_dim) for _ in range(hidden_layers)], # *[]将循环得到的解压\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 超参数定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred><b>***TODO***:  可以考虑进一步优化超参数来提高准确率。</font></b>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:13:09.512866Z",
     "start_time": "2024-10-07T14:13:09.500902Z"
    }
   },
   "source": [
    "# data prarameters\n",
    "# 用于数据处理时的参数\n",
    "concat_nframes = 1              # 要连接的帧数,n必须为奇数（总共2k+1=n帧）\n",
    "train_ratio = 0.8               # 用于训练的数据比率，其余数据将用于验证\n",
    "# training parameters\n",
    "# 训练过程中的参数\n",
    "seed = 0                        # 随机种子\n",
    "batch_size = 512                # 批次数目\n",
    "num_epoch = 5                   # 训练epoch数\n",
    "learning_rate = 0.0001          # 学习率\n",
    "model_path = './model.ckpt'     # 选择保存检查点的路径（即下文调用保存模型函数的保存位置）\n",
    "# model parameters\n",
    "# 模型参数\n",
    "input_dim = 39 * concat_nframes # 模型的输入维度，不应更改该值，这个值由上面的拼接函数决定\n",
    "hidden_layers = 1               # hidden_layer层的数量\n",
    "hidden_dim = 256                # 隐藏维度"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据与模型"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:20:14.700596Z",
     "start_time": "2024-10-07T14:19:59.308063Z"
    }
   },
   "source": [
    "# 引入gc模块进行垃圾回收\n",
    "import gc\n",
    "\n",
    "# 预处理数据\n",
    "train_X, train_y = preprocess_data(split='train', feat_dir='./libriphone/feat', phone_path='./libriphone', concat_nframes=concat_nframes, train_ratio=train_ratio)\n",
    "val_X, val_y = preprocess_data(split='val', feat_dir='./libriphone/feat', phone_path='./libriphone', concat_nframes=concat_nframes, train_ratio=train_ratio)\n",
    "\n",
    "# 将数据导入\n",
    "train_set = LibriDataset(train_X, train_y)\n",
    "val_set = LibriDataset(val_X, val_y)\n",
    "\n",
    "# 删除原始数据以节省内存\n",
    "del train_X, train_y, val_X, val_y\n",
    "gc.collect()\n",
    "\n",
    "# 利用dataloader加载数据\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] - # phone classes: 41, number of utterances for train: 3428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]C:\\Users\\18705\\AppData\\Local\\Temp\\ipykernel_11852\\1703510744.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  feat = torch.load(path)\n",
      "3428it [00:08, 407.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] train set\n",
      "torch.Size([2116368, 39])\n",
      "torch.Size([2116368])\n",
      "[Dataset] - # phone classes: 41, number of utterances for val: 858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "858it [00:02, 378.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] val set\n",
      "torch.Size([527790, 39])\n",
      "torch.Size([527790])\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:20:32.026783Z",
     "start_time": "2024-10-07T14:20:31.968941Z"
    }
   },
   "source": [
    "# 检查当前是否有可用的GPU 否则使用CPU\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'DEVICE: {device}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda:0\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:20:34.747539Z",
     "start_time": "2024-10-07T14:20:34.731581Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "# 固定随机种子\n",
    "def same_seeds(seed): # 固定随机种子（CPU）\n",
    "    torch.manual_seed(seed) # 固定随机种子（GPU)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed) # 为当前GPU设置\n",
    "        torch.cuda.manual_seed_all(seed)  # 为所有GPU设置\n",
    "    np.random.seed(seed)  # 保证后续使用random函数时，产生固定的随机数\n",
    "    torch.backends.cudnn.benchmark = False # GPU、网络结构固定，可设置为True\n",
    "    torch.backends.cudnn.deterministic = True # 固定网络结构"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:20:45.077982Z",
     "start_time": "2024-10-07T14:20:41.424759Z"
    }
   },
   "source": [
    "# 固定随机种子\n",
    "same_seeds(seed)\n",
    "\n",
    "# 创建模型、定义损失函数和优化器\n",
    "model = Classifier(input_dim=input_dim, hidden_layers=hidden_layers, hidden_dim=hidden_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:25:42.170269Z",
     "start_time": "2024-10-07T14:20:54.567323Z"
    }
   },
   "source": [
    "best_acc = 0.0\n",
    "for epoch in range(num_epoch):\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    # 训练部分\n",
    "    model.train() # 设定模型到训练模式\n",
    "    for i, batch in enumerate(tqdm(train_loader)):\n",
    "        features, labels = batch\n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad() \n",
    "        outputs = model(features) \n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "        \n",
    "        _, train_pred = torch.max(outputs, 1) # 获得概率最高的类的索引\n",
    "        train_acc += (train_pred.detach() == labels.detach()).sum().item()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # 验证部分\n",
    "    if len(val_set) > 0:\n",
    "        model.eval() # 设定模型到评估模式\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(tqdm(val_loader)):\n",
    "                features, labels = batch\n",
    "                features = features.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(features)\n",
    "                \n",
    "                loss = criterion(outputs, labels) \n",
    "                \n",
    "                _, val_pred = torch.max(outputs, 1) \n",
    "                val_acc += (val_pred.cpu() == labels.cpu()).sum().item() # 获得概率最高的类的索引\n",
    "                val_loss += loss.item()\n",
    "\n",
    "            print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f} | Val Acc: {:3.6f} loss: {:3.6f}'.format(\n",
    "                epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader), val_acc/len(val_set), val_loss/len(val_loader)\n",
    "            ))\n",
    "\n",
    "            # 如果模型获得提升，在此阶段保存模型\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                print('saving model with acc {:.3f}'.format(best_acc/len(val_set)))\n",
    "    else:\n",
    "        print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f}'.format(\n",
    "            epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader)\n",
    "        ))\n",
    "\n",
    "# 如果结束验证，则保存最后一个epoch得到的模型\n",
    "if len(val_set) == 0:\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print('saving model at last epoch')\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4134/4134 [00:52<00:00, 78.94it/s]\n",
      "100%|██████████| 1031/1031 [00:08<00:00, 120.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/005] Train Acc: 0.421904 Loss: 2.086648 | Val Acc: 0.440577 loss: 1.971936\n",
      "saving model with acc 0.441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4134/4134 [00:47<00:00, 87.04it/s]\n",
      "100%|██████████| 1031/1031 [00:08<00:00, 118.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[002/005] Train Acc: 0.449007 Loss: 1.934501 | Val Acc: 0.449685 loss: 1.926882\n",
      "saving model with acc 0.450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4134/4134 [00:51<00:00, 80.48it/s]\n",
      "100%|██████████| 1031/1031 [00:08<00:00, 120.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[003/005] Train Acc: 0.455076 Loss: 1.904380 | Val Acc: 0.453830 loss: 1.908920\n",
      "saving model with acc 0.454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4134/4134 [00:46<00:00, 89.40it/s]\n",
      "100%|██████████| 1031/1031 [00:08<00:00, 126.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[004/005] Train Acc: 0.458392 Loss: 1.888021 | Val Acc: 0.456157 loss: 1.896542\n",
      "saving model with acc 0.456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4134/4134 [00:47<00:00, 86.86it/s]\n",
      "100%|██████████| 1031/1031 [00:08<00:00, 122.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[005/005] Train Acc: 0.460759 Loss: 1.876457 | Val Acc: 0.457879 loss: 1.889787\n",
      "saving model with acc 0.458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:25:52.010931Z",
     "start_time": "2024-10-07T14:25:51.906213Z"
    }
   },
   "source": [
    "del train_loader, val_loader\n",
    "gc.collect()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试\n",
    "创建测试数据集，并从保存的检查点加载模型。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:25:56.781499Z",
     "start_time": "2024-10-07T14:25:55.592107Z"
    }
   },
   "source": [
    "# 载入数据\n",
    "test_X = preprocess_data(split='test', feat_dir='./libriphone/feat', phone_path='./libriphone', concat_nframes=concat_nframes)\n",
    "test_set = LibriDataset(test_X, None)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] - # phone classes: 41, number of utterances for test: 1078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]C:\\Users\\18705\\AppData\\Local\\Temp\\ipykernel_11852\\1703510744.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  feat = torch.load(path)\n",
      "1078it [00:01, 921.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] test set\n",
      "torch.Size([646268, 39])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:26:01.802028Z",
     "start_time": "2024-10-07T14:26:01.766085Z"
    }
   },
   "source": [
    "# 加载已经训练好的模型\n",
    "model = Classifier(input_dim=input_dim, hidden_layers=hidden_layers, hidden_dim=hidden_dim).to(device)\n",
    "model.load_state_dict(torch.load(model_path))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\18705\\AppData\\Local\\Temp\\ipykernel_11852\\2924126090.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:26:15.337854Z",
     "start_time": "2024-10-07T14:26:08.668140Z"
    }
   },
   "source": [
    "test_acc = 0.0\n",
    "test_lengths = 0\n",
    "pred = np.array([], dtype=np.int32)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(tqdm(test_loader)):\n",
    "        features = batch\n",
    "        features = features.to(device)\n",
    "\n",
    "        outputs = model(features)\n",
    "\n",
    "        _, test_pred = torch.max(outputs, 1) # 获得概率最高的类的索引\n",
    "        pred = np.concatenate((pred, test_pred.cpu().numpy()), axis=0)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1263/1263 [00:06<00:00, 189.70it/s]\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将预测结果写入CSV文件。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:26:19.706880Z",
     "start_time": "2024-10-07T14:26:17.841838Z"
    }
   },
   "source": [
    "with open('prediction.csv', 'w') as f:\n",
    "    f.write('Id,Class\\n')\n",
    "    for i, y in enumerate(pred):\n",
    "        f.write('{},{}\\n'.format(i, y))"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献：  \n",
    "[一文入门dropout层](https://www.cnblogs.com/geeksongs/p/13446980.html)  \n",
    "李宏毅机器学习2022在线课程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 贡献者  \n",
    "潘笃驿(panduyi_azula@foxmail.com)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "5da60e2a9548b1451378568c9bbd9389177ea512d36b61a0763bf3d806b69d96"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
